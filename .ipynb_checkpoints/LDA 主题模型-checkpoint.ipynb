{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 频率派把需要推断的参数θ看做是固定的未知常数，即概率 $\\theta$ 虽然是未知的，但最起码是确定的一个值，同时，样本 X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本 X 的分布；\n",
    "- 而贝叶斯派的观点则截然相反，他们认为待估计的参数 $\\theta$ 是随机变量，服从一定的分布，而样本 X 是固定的，由于样本是固定的，所以他们重点研究的是参数$\\theta$的分布。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pLSA 模型 (Probabilistic Latent Semantic Analysis)\n",
    "频率学派，pLSA主要是运用了EM的思想，分为两步，\n",
    "\n",
    "- E步骤：求隐含变量 Given 当前估计的参数条件下的后验概率\n",
    "- M步骤：最大化 Complete data 对数似然函数的期望，此时我们使用 E 步骤里计算的隐含变量的后验概率，得到新的参数值。\n",
    "\n",
    "在pLAS算发中，E步骤是：直接使用贝叶斯公式计算隐含变量在当前参数取值条件下的后验概率，有\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/90720ae7e5c6f3b0883848ae798d31bde4f9876a)\n",
    "\n",
    "M步骤是求下面式子的最大似然：\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/eab83754b6b1fe1d45db1f09816d60719f891f41)\n",
    "\n",
    "通过一些算法，可以更新新值：\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/7cc764145f18df1a6f1cee1ddd1b51c57219117d)\n",
    "\n",
    "下面根据上面的公式来进行代码实现，代码来自：https://github.com/laserwave/PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import zeros, int8, log\n",
    "from pylab import random\n",
    "import sys\n",
    "import jieba\n",
    "import re\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 预处理文档\n",
    "def preprocessing(datasetFilePath, stopwordsFilePath):\n",
    "    \n",
    "    # read the stopwords file\n",
    "    file = codecs.open(stopwordsFilePath, 'r', 'utf-8')\n",
    "    stopwords = [line.strip() for line in file] \n",
    "    file.close()\n",
    "    \n",
    "    # read the documents\n",
    "    file = codecs.open(datasetFilePath, 'r', 'utf-8')\n",
    "    documents = [document.strip() for document in file] \n",
    "    file.close()\n",
    "\n",
    "    # number of documents\n",
    "    N = len(documents)\n",
    "\n",
    "    wordCounts = [];\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    currentId = 0;\n",
    "    # generate the word2id and id2word maps and count the number of times of words showing up in documents\n",
    "    for document in documents:\n",
    "        # 结巴分词，去除数字和停止词\n",
    "        segList = jieba.cut(document)\n",
    "        wordCount = {}\n",
    "        for word in segList:\n",
    "            word = word.lower().strip()\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:               \n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = currentId;\n",
    "                    id2word[currentId] = word;\n",
    "                    currentId += 1;\n",
    "                if word in wordCount:\n",
    "                    wordCount[word] += 1\n",
    "                else:\n",
    "                    wordCount[word] = 1\n",
    "        # 每个document的单词数\n",
    "        wordCounts.append(wordCount);\n",
    "    \n",
    "    # length of dictionary\n",
    "    M = len(word2id)  \n",
    "\n",
    "    # generate the document-word matrix\n",
    "    X = zeros([N, M], int8)\n",
    "    for word in word2id.keys():\n",
    "        j = word2id[word]\n",
    "        for i in range(0, N):\n",
    "            if word in wordCounts[i]:\n",
    "                X[i, j] = wordCounts[i][word];    \n",
    "\n",
    "    return N, M, word2id, id2word, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10    # number of topic\n",
    "datasetFilePath = './data/dataset3.txt'\n",
    "stopwordsFilePath = './data/stopwords.dic'\n",
    "# preprocessing\n",
    "N, M, word2id, id2word, X = preprocessing(datasetFilePath, stopwordsFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 5757\n"
     ]
    }
   ],
   "source": [
    "print(N,M) # 50篇文档，5057个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5757"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lamda[i, j] : p(zj|di)\n",
    "lamda = random([N, K])  # [N,K] 每一行都是一个文档，列是主题 每篇文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theta[i, j] : p(wj|zi)\n",
    "theta = random([K, M]) # [K,M] 每一行都是一个主题，列是单词 每个主题的词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p[i, j, k] : p(zk|di,wj) 后验概率\n",
    "p = zeros([N, M, K]) # N 篇文档 M 个单词 K 个主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2302121533617125"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lamda[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 归一 p(zj|di) 和 p(wj|zi)\n",
    "def initializeParameters():\n",
    "    for i in range(0, N):\n",
    "        normalization = sum(lamda[i, :])\n",
    "        for j in range(0, K):\n",
    "            lamda[i, j] /= normalization\n",
    "\n",
    "    for i in range(0, K):\n",
    "        normalization = sum(theta[i, :])\n",
    "        for j in range(0, M):\n",
    "            theta[i, j] /= normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializeParameters()\n",
    "sum(lamda[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/90720ae7e5c6f3b0883848ae798d31bde4f9876a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EStep():\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            denominator = 0;\n",
    "            for k in range(0, K):\n",
    "                p[i, j, k] = theta[k, j] * lamda[i, k];\n",
    "                denominator += p[i, j, k];\n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] = 0;\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    p[i, j, k] /= denominator;                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/7cc764145f18df1a6f1cee1ddd1b51c57219117d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MStep():\n",
    "    # update theta\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        for j in range(0, M):\n",
    "            theta[k, j] = 0\n",
    "            for i in range(0, N):\n",
    "                theta[k, j] += X[i, j] * p[i, j, k]\n",
    "            denominator += theta[k, j]\n",
    "        if denominator == 0:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] = 1.0 / M\n",
    "        else:\n",
    "            for j in range(0, M):\n",
    "                theta[k, j] /= denominator\n",
    "        \n",
    "    # update lamda\n",
    "    for i in range(0, N):\n",
    "        for k in range(0, K):\n",
    "            lamda[i, k] = 0\n",
    "            denominator = 0\n",
    "            for j in range(0, M):\n",
    "                lamda[i, k] += X[i, j] * p[i, j, k]\n",
    "                denominator += X[i, j];\n",
    "            if denominator == 0:\n",
    "                lamda[i, k] = 1.0 / K\n",
    "            else:\n",
    "                lamda[i, k] /= denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/64a167f39370d46555cea0b0b5469a3338a4c2e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the log likelihood\n",
    "def LogLikelihood():\n",
    "    loglikelihood = 0\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            tmp = 0\n",
    "            for k in range(0, K):\n",
    "                tmp += theta[k, j] * lamda[i, k]\n",
    "            if tmp > 0:\n",
    "                loglikelihood += X[i, j] * log(tmp)\n",
    "    return loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的训练方法简直属于无法忍受。太慢了。所以这种模型如果在生产环境中运用，简直了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2017-05-11 17:40:53 ]  1  iteration   -119253.693308\n",
      "[ 2017-05-11 17:41:19 ]  2  iteration   -117125.295154\n",
      "[ 2017-05-11 17:41:45 ]  3  iteration   -114139.544231\n",
      "[ 2017-05-11 17:42:11 ]  4  iteration   -110593.442828\n",
      "[ 2017-05-11 17:42:39 ]  5  iteration   -107314.644734\n",
      "[ 2017-05-11 17:43:04 ]  6  iteration   -104791.838112\n",
      "[ 2017-05-11 17:43:29 ]  7  iteration   -102955.824843\n",
      "[ 2017-05-11 17:43:54 ]  8  iteration   -101589.587169\n",
      "[ 2017-05-11 17:44:19 ]  9  iteration   -100580.275362\n",
      "[ 2017-05-11 17:44:43 ]  10  iteration   -99825.3994997\n",
      "[ 2017-05-11 17:45:09 ]  11  iteration   -99226.3883371\n",
      "[ 2017-05-11 17:45:34 ]  12  iteration   -98705.058356\n",
      "[ 2017-05-11 17:45:59 ]  13  iteration   -98251.078876\n",
      "[ 2017-05-11 17:46:25 ]  14  iteration   -97910.6944839\n",
      "[ 2017-05-11 17:46:49 ]  15  iteration   -97686.4386685\n",
      "[ 2017-05-11 17:47:15 ]  16  iteration   -97558.8019102\n",
      "[ 2017-05-11 17:47:44 ]  17  iteration   -97491.4860489\n",
      "[ 2017-05-11 17:48:13 ]  18  iteration   -97444.8254177\n",
      "[ 2017-05-11 17:48:44 ]  19  iteration   -97403.6639548\n",
      "[ 2017-05-11 17:49:11 ]  20  iteration   -97370.3761201\n",
      "[ 2017-05-11 17:49:40 ]  21  iteration   -97347.9686148\n",
      "[ 2017-05-11 17:50:10 ]  22  iteration   -97336.3725504\n",
      "[ 2017-05-11 17:50:41 ]  23  iteration   -97326.6404064\n"
     ]
    }
   ],
   "source": [
    "maxIteration = 30\n",
    "threshold = 10.0\n",
    "# EM algorithm\n",
    "oldLoglikelihood = 1\n",
    "newLoglikelihood = 1\n",
    "for i in range(0, maxIteration):\n",
    "    EStep()\n",
    "    MStep()\n",
    "    newLoglikelihood = LogLikelihood()\n",
    "    print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "        break\n",
    "    oldLoglikelihood = newLoglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docTopicDist = './data/docTopicDistribution.txt'\n",
    "topicWordDist = './data/topicWordDistribution.txt'\n",
    "dictionary = './data/dictionary.dic'\n",
    "topicWords = './data/topics.txt'\n",
    "topicWordsNum = 10\n",
    "# output the params of model and top words of topics to files\n",
    "def output():\n",
    "    # document-topic distribution\n",
    "    file = codecs.open(docTopicDist,'w','utf-8')\n",
    "    for i in range(0, N):\n",
    "        tmp = ''\n",
    "        for j in range(0, K):\n",
    "            tmp += str(lamda[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # topic-word distribution\n",
    "    file = codecs.open(topicWordDist,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        tmp = ''\n",
    "        for j in range(0, M):\n",
    "            tmp += str(theta[i, j]) + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # dictionary\n",
    "    file = codecs.open(dictionary,'w','utf-8')\n",
    "    for i in range(0, M):\n",
    "        file.write(id2word[i] + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "    # top words of each topic\n",
    "    file = codecs.open(topicWords,'w','utf-8')\n",
    "    for i in range(0, K):\n",
    "        topicword = []\n",
    "        ids = theta[i, :].argsort()\n",
    "        for j in ids:\n",
    "            topicword.insert(0, id2word[j])\n",
    "        tmp = ''\n",
    "        for word in topicword[0:min(topicWordsNum, len(topicword))]:\n",
    "            tmp += word + ' '\n",
    "        file.write(tmp + '\\n')\n",
    "    file.close()\n",
    "\n",
    "output()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 模型\n",
    "贝叶斯学派\n",
    "LDA 就是在 pLSA 的基础上加层贝叶斯框架，即 LDA 就是 pLSA 的贝叶斯版本（正因为 LDA 被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。\n",
    "\n",
    "[原理](http://blog.csdn.net/v_july_v/article/details/41209515)\n",
    "\n",
    "以下代码取自：[Topic Modeling with Scikit Learn](https://medium.com/@aneesha/topic-modeling-with-scikit-learn-e80d33668730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# 获取语料\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "Well i'm not sure about the story nad it did seem biased. What\n",
      "I disagree with is your statement that the U.S. Media is out to\n",
      "ruin Israels reputation. That is rediculous. The U.S. media is\n",
      "the most pro-israeli media in the world. Having lived in Europe\n",
      "I realize that incidences such as the one described in the\n",
      "letter have occured. The U.S. media as a whole seem to try to\n",
      "ignore them. The U.S. is subsidizing Israels existance and the\n",
      "Europeans are not (at least not to the same degree). So I think\n",
      "that might be a reason they report more clearly on the\n",
      "atrocities.\n",
      "\tWhat is a shame is that in Austria, daily reports of\n",
      "the inhuman acts commited by Israeli soldiers and the blessing\n",
      "received from the Government makes some of the Holocaust guilt\n",
      "go away. After all, look how the Jews are treating other races\n",
      "when they got power. It is unfortunate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(documents)) # list\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tfidf和简单计数两种方法来处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_feature_names),len(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个方法都必须给定topic的数量，两个算法都不能自动识别出来\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n",
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步我们需要做的是，怎么知道每个topic其具体的含义\n",
    "\n",
    "有一个[R package for interactive topic model visualization](https://github.com/cpsievert/LDAvis)，一个主题的可视化工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结巴分词和LDA结合\n",
    "我们先来一点简单的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"我来到北京清华大学\",\n",
    "        \"他来到了网易杭研大厦\",\n",
    "        \"小明硕士毕业与中国科学院\",\n",
    "        \"我爱北京天安门\"]\n",
    "\n",
    "corpus_after = []\n",
    "for cor in corpus:\n",
    "    segList = jieba.cut(cor)\n",
    "    corpus_after.append(\" \".join(segList))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我 来到 北京 清华大学', '他 来到 了 网易 杭研 大厦', '小明 硕士 毕业 与 中国科学院', '我 爱 北京 天安门']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(corpus_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "transformer= TfidfTransformer()#该类会统计每个词语的 tf-idf 权值  \n",
    "tfidf=transformer.fit_transform(tf)#第一个 fit_transform 是计算 tf-idf，第二个 fit_transform 是将文本转为词频矩阵  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- 这里输出第 0 类文本的词语 tf-idf 权重 ------\n",
      "\n",
      "中国科学院 0.0\n",
      "北京 0.52640543361\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.52640543361\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.66767854461\n",
      "硕士 0.0\n",
      "网易 0.0\n",
      "------- 这里输出第 1 类文本的词语 tf-idf 权重 ------\n",
      "\n",
      "中国科学院 0.0\n",
      "北京 0.0\n",
      "大厦 0.525472749264\n",
      "天安门 0.0\n",
      "小明 0.0\n",
      "来到 0.414288751166\n",
      "杭研 0.525472749264\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "网易 0.525472749264\n",
      "------- 这里输出第 2 类文本的词语 tf-idf 权重 ------\n",
      "\n",
      "中国科学院 0.5\n",
      "北京 0.0\n",
      "大厦 0.0\n",
      "天安门 0.0\n",
      "小明 0.5\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.5\n",
      "清华大学 0.0\n",
      "硕士 0.5\n",
      "网易 0.0\n",
      "------- 这里输出第 3 类文本的词语 tf-idf 权重 ------\n",
      "\n",
      "中国科学院 0.0\n",
      "北京 0.61913029649\n",
      "大厦 0.0\n",
      "天安门 0.78528827571\n",
      "小明 0.0\n",
      "来到 0.0\n",
      "杭研 0.0\n",
      "毕业 0.0\n",
      "清华大学 0.0\n",
      "硕士 0.0\n",
      "网易 0.0\n"
     ]
    }
   ],
   "source": [
    "weight=tfidf.toarray()#将 tf-idf 矩阵抽取出来，元素 a[i][j] 表示 j 词在 i 类文本中的 tf-idf 权重  \n",
    "for i in range(len(weight)):#打印每类文本的 tf-idf 词语权重，第一个 for 遍历所有文本，第二个 for 便利某一类文本下的词语权重  \n",
    "    print(u\"------- 这里输出第\",i,u\"类文本的词语 tf-idf 权重 ------\\n\")\n",
    "    for j in range(len(word)):  \n",
    "        print(word[j],weight[i][j])\n",
    "# 此处我们丢失了词的位置信息        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
