{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种概念\n",
    "\n",
    "\n",
    "### GBDT\n",
    "GBDT的中文释义是：梯度提升决策树。GBDT 主要由三个概念组成：\n",
    "\n",
    "- Regression Decistion Tree（即 DT)\n",
    "- Gradient Boosting（即 GB)\n",
    "- Shrinkage (算法的一个重要演进分枝，目前大部分源码都按该版本实现）\n",
    "\n",
    "首先回归决策树（DT），回归分析与分类不同在于 loss 函数的不同。\n",
    "![](http://scikit-learn.org/stable/_images/math/18b1a49e164e731a75bb626f23ecce70fdf0a181.png)\n",
    "\n",
    "接着是梯度提升（GB），这边讲到 boosting ，就不得不提 bagging ，而 Bagging 和 Boosting 是集成学习下有两个重要的策略。\n",
    "\n",
    "来源[为什么 xgboost/gbdt 在调参时为什么树的深度很少就能达到很高的精度？](https://www.zhihu.com/question/45487317)\n",
    "Bagging 算法是这样做的：每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。其代表算法是随机森林。Boosting 的意思是这样，他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。其代表算法是 AdaBoost, GBDT。\n",
    "\n",
    "然后另一个重要的概念是：模型的复杂度和泛化误差，泛化误差可以分为两部分，偏差（bias) 和方差 (variance)。\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/2852c3de5a904452bdb2eb586b567ab576d5c663)\n",
    "\n",
    "那到底什么是boost，我们可以来看个例子\n",
    "![](http://bos.nj.bpc.baidu.com/v1/agroup/878b13054499ba326113f62702708c9987b6b6cf)\n",
    "\n",
    "boost算法每次都用上次的残差作为输入进行拟合，力求的是偏差最小，因此每课树的深度不会太深（防止方差变大，过拟合）。\n",
    "\n",
    "### GBDT 算法实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "                                max_depth=4, random_state=0, loss='ls').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4224112008293304"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost算法解释\n",
    "AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类学习方法。\n",
    "\n",
    "\n",
    "[机器学习算法中 GBDT 与 Adaboost 的区别与联系是什么？](https://www.zhihu.com/question/54626685)\n",
    "\n",
    "一篇论文[The Evolution of Boosting Algorithms From Machine Learning to Statistical Modelling](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1403.1452.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文章\n",
    "[GBDT：梯度提升决策树](http://www.jianshu.com/p/005a4e6ac775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
